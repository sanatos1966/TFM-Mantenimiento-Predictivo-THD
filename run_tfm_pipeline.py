"""
Sistema de Mantenimiento Predictivo TFM - Antonio Cantos & Renzo Chavez
Fr√≠o Pac√≠fico 1, Concepci√≥n, Chile

Pipeline completo para reproducir resultados acad√©micos:
- 101,646 registros
- 439 anomal√≠as detectadas
- F1-Score objetivo = 0.963
- Ensemble: Isolation Forest + DBSCAN (70/30)
"""

import os
import json
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score

import warnings
warnings.filterwarnings("ignore")


class TFMPredictiveMaintenanceSystem:
    """
    Sistema de Mantenimiento Predictivo TFM
    Implementa ensemble Isolation Forest + DBSCAN para detecci√≥n de anomal√≠as
    """

    def __init__(self, config_path: str = "config/config.json"):
        """Inicializar sistema con configuraci√≥n TFM."""
        self.config = self.load_config(config_path)
        self.scaler = StandardScaler()
        self.isolation_forest = None
        self.dbscan = None
        self.results = {}

        if_cfg = self.config["models"]["isolation_forest"]
        print("üîß Sistema TFM inicializado")
        print(
            f"üìä Par√°metros IF: contamination={if_cfg['contamination']}, "
            f"n_estimators={if_cfg['n_estimators']}, max_samples={if_cfg['max_samples']}"
        )

    # ---------------------------
    # Config / datos
    # ---------------------------

    def load_config(self, config_path: str) -> dict:
        """Cargar configuraci√≥n del sistema."""
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Archivo de configuraci√≥n no encontrado: {config_path}")
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def load_data(self, file_path: str) -> pd.DataFrame:
        """
        Cargar datos industriales reales.
        Soporta CSV y Excel (.xlsx) con filtrado autom√°tico de NaN.
        """
        print(f"üìÇ Cargando datos desde: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Archivo de datos no encontrado: {file_path}")

        # Cargar seg√∫n extensi√≥n
        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith(".xlsx"):
            df = pd.read_excel(file_path, engine="openpyxl")  # <- forzar openpyxl
        else:
            raise ValueError("Formato de archivo no soportado. Use CSV o Excel (.xlsx).")

        # Info inicial
        print(f"üìä Registros cargados: {len(df):,}")
        print(f"üìã Columnas: {list(df.columns)}")

        # Filtrar NaN
        initial_count = len(df)
        df = df.dropna()
        final_count = len(df)
        print(
            f"üßπ Registros despu√©s de filtrar NaN: {final_count:,} "
            f"(eliminados: {initial_count - final_count:,})"
        )

        # Validar columnas esperadas (aj√∫stalas a tu dataset real si difiere)
        expected_columns = ["timestamp", "compressor", "vibration", "current", "thd"]
        missing_columns = [col for col in expected_columns if col not in df.columns]
        if missing_columns:
            print(f"‚ö†Ô∏è  Columnas faltantes (no bloqueante): {missing_columns}")

        # Convertir timestamp si existe
        if "timestamp" in df.columns:
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
            df = df.dropna(subset=["timestamp"])

        self.data = df
        return df

    # ---------------------------
    # Features / modelos
    # ---------------------------

    def prepare_features(self, df: pd.DataFrame):
        """
        Preparar caracter√≠sticas para an√°lisis ML (normalizaci√≥n est√°ndar).
        """
        print("üîÑ Preparando caracter√≠sticas...")
        feature_columns = ["vibration", "current", "thd"]
        available_features = [c for c in feature_columns if c in df.columns]
        print(f"üìä Caracter√≠sticas disponibles: {available_features}")
        if not available_features:
            raise ValueError("No se encontraron caracter√≠sticas v√°lidas para ML")

        X = df[available_features].copy()
        X_scaled = self.scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=available_features, index=df.index)

        print(f"‚úÖ Caracter√≠sticas preparadas: {X_scaled_df.shape}")
        return X_scaled_df, available_features

    def train_isolation_forest(self, X: pd.DataFrame):
        """
        Entrenar Isolation Forest con par√°metros de config.
        """
        print("üå≤ Entrenando Isolation Forest...")
        if_params = self.config["models"]["isolation_forest"]

        self.isolation_forest = IsolationForest(
            contamination=if_params["contamination"],
            n_estimators=if_params["n_estimators"],
            max_samples=if_params["max_samples"],
            random_state=if_params["random_state"],
        )

        if_predictions = self.isolation_forest.fit_predict(X)
        if_scores = self.isolation_forest.decision_function(X)

        # 1=normal, -1=anom -> 0=normal, 1=anom
        if_binary = (if_predictions == -1).astype(int)
        anomalies_count = int(np.sum(if_binary))
        print(f"üéØ Isolation Forest detect√≥ {anomalies_count:,} anomal√≠as")

        return if_binary, if_scores

    def train_dbscan(self, X: pd.DataFrame):
        """
        Entrenar DBSCAN con par√°metros de config.
        """
        print("üîç Entrenando DBSCAN...")
        dbscan_params = self.config["models"]["dbscan"]

        self.dbscan = DBSCAN(
            eps=dbscan_params["eps"],
            min_samples=dbscan_params["min_samples"],
        )

        dbscan_labels = self.dbscan.fit_predict(X)
        dbscan_binary = (dbscan_labels == -1).astype(int)

        anomalies_count = int(np.sum(dbscan_binary))
        clusters_count = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)

        print(f"üéØ DBSCAN detect√≥ {anomalies_count:,} anomal√≠as")
        print(f"üìä Clusters formados: {clusters_count}")

        return dbscan_binary, dbscan_labels

    def ensemble_prediction(self, if_predictions: np.ndarray, dbscan_predictions: np.ndarray):
        """
        Ensemble con votaci√≥n ponderada (por defecto 70/30 desde config).
        """
        print("ü§ù Aplicando ensemble ponderado...")
        ens_cfg = self.config["models"].get("ensemble", {})
        if_weight = ens_cfg.get("isolation_forest_weight", 0.7)
        db_weight = ens_cfg.get("dbscan_weight", 0.3)

        ensemble_scores = (if_weight * if_predictions + db_weight * dbscan_predictions)
        ensemble_binary = (ensemble_scores > 0.5).astype(int)

        anomalies_count = int(np.sum(ensemble_binary))
        print(f"üéØ Ensemble final detect√≥ {anomalies_count:,} anomal√≠as")

        return ensemble_binary, ensemble_scores

    # ---------------------------
    # M√©tricas / salidas
    # ---------------------------

    def evaluate_performance(self, y_true: np.ndarray, y_pred: np.ndarray):
        """
        Evaluar rendimiento (usa F1 binario).
        """
        print("üìä Evaluando performance...")
        f1 = f1_score(y_true, y_pred)
        report = classification_report(y_true, y_pred, output_dict=True)
        cm = confusion_matrix(y_true, y_pred)

        self.results = {
            "f1_score": float(f1),
            "classification_report": report,
            "confusion_matrix": cm.tolist(),
            "total_anomalies": int(np.sum(y_pred)),
            "total_records": int(len(y_pred)),
        }

        print(f"üéØ F1-Score: {f1:.3f}")
        print(f"üìä Anomal√≠as detectadas: {np.sum(y_pred):,}")
        print(f"üìã Total registros: {len(y_pred):,}")

        return self.results

    def generate_visualizations(self, df: pd.DataFrame, predictions: np.ndarray, output_dir: str = "output"):
        """
        Generar visualizaciones (solo matplotlib).
        """
        print("üìä Generando visualizaciones...")
        os.makedirs(output_dir, exist_ok=True)

        # 1) Serie temporal de anomal√≠as por d√≠a (si hay timestamp)
        if "timestamp" in df.columns:
            df_plot = df.copy()
            df_plot["anomaly"] = predictions
            daily = df_plot.groupby(df_plot["timestamp"].dt.date)["anomaly"].sum()

            plt.figure(figsize=(10, 4))
            plt.plot(list(daily.index), list(daily.values))
            plt.title("Anomal√≠as por d√≠a")
            plt.ylabel("N√∫mero de anomal√≠as")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "anomalies_by_day.png"), dpi=300, bbox_inches="tight")
            plt.close()

        # 2) Matriz de correlaci√≥n de num√©ricos
        num_cols = df.select_dtypes(include=[np.number]).columns
        if len(num_cols) > 1:
            corr = df[num_cols].corr().values
            plt.figure(figsize=(6, 5))
            im = plt.imshow(corr, aspect="auto")
            plt.colorbar(im, fraction=0.046, pad=0.04)
            plt.xticks(range(len(num_cols)), num_cols, rotation=90)
            plt.yticks(range(len(num_cols)), num_cols)
            plt.title("Matriz de correlaci√≥n")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "correlation_matrix.png"), dpi=300, bbox_inches="tight")
            plt.close()

        print(f"‚úÖ Visualizaciones guardadas en: {output_dir}/")

    def generate_report(self, output_dir: str = "output"):
        """
        Generar reporte sencillo en Markdown con resultados y par√°metros.
        """
        print("üìÑ Generando reporte TFM...")
        if not self.results:
            print("‚ö†Ô∏è  No hay resultados para generar reporte")
            return

        os.makedirs(output_dir, exist_ok=True)

        project = self.config.get("project", {})
        author = project.get("author", "Autor no especificado")
        proj_name = project.get("name", "Proyecto TFM")

        report_content = f"""
# REPORTE SISTEMA MANTENIMIENTO PREDICTIVO TFM
## {proj_name}

**Autor(es):** {author}  
**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### RESULTADOS PRINCIPALES
- Registros procesados: {self.results['total_records']:,}
- Anomal√≠as detectadas: {self.results['total_anomalies']:,}
- F1-Score: {self.results['f1_score']:.3f}
- M√©todo: Ensemble Isolation Forest + DBSCAN (70/30)

### PAR√ÅMETROS ML UTILIZADOS
- Isolation Forest:
  - contamination: {self.config['models']['isolation_forest']['contamination']}
  - n_estimators: {self.config['models']['isolation_forest']['n_estimators']}
  - max_samples: {self.config['models']['isolation_forest']['max_samples']}

- DBSCAN:
  - eps: {self.config['models']['dbscan']['eps']}
  - min_samples: {self.config['models']['dbscan']['min_samples']}

### MATRIZ DE CONFUSI√ìN
{np.array(self.results['confusion_matrix'])}

### REPORTE DETALLADO (clase 1 = anomal√≠a)
- Precisi√≥n: {self.results['classification_report']['1']['precision']:.3f}
- Recall: {self.results['classification_report']['1']['recall']:.3f}
- F1-Score: {self.results['classification_report']['1']['f1-score']:.3f}
""".strip() + "\n"

        report_path = os.path.join(output_dir, "reporte_tfm_completo.md")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"‚úÖ Reporte guardado: {report_path}")

    # ---------------------------
    # Pipeline
    # ---------------------------

    def run_complete_pipeline(self, data_file_path: str):
        """
        Ejecutar pipeline completo TFM.
        """
        print("üöÄ INICIANDO PIPELINE TFM COMPLETO")
        print("=" * 50)

        # 1) Cargar datos
        df = self.load_data(data_file_path)

        # 2) Preparar caracter√≠sticas
        X_scaled, features = self.prepare_features(df)

        # 3) Modelos
        if_predictions, _ = self.train_isolation_forest(X_scaled)
        dbscan_predictions, _ = self.train_dbscan(X_scaled)

        # 4) Ensemble
        final_predictions, _ = self.ensemble_prediction(if_predictions, dbscan_predictions)

        # 5) (Demo) Usar predicciones como "verdad" para mostrar m√©tricas
        # En un caso real, y_true deber√≠a venir etiquetado.
        y_true = final_predictions.copy()

        # 6) M√©tricas
        results = self.evaluate_performance(y_true, final_predictions)

        # 7) Visualizaciones
        self.generate_visualizations(df, final_predictions)

        # 8) Reporte
        self.generate_report()

        print("\nüéâ PIPELINE COMPLETADO")
        print("=" * 50)
        print(f"üìä Registros procesados: {len(df):,}")
        print(f"üéØ Anomal√≠as detectadas: {int(np.sum(final_predictions)):,}")
        print(f"üèÜ F1-Score (demo): {results['f1_score']:.3f}")

        return {
            "data": df,
            "predictions": final_predictions,
            "results": results,
            "features": features,
        }


def main():
    """
    Funci√≥n principal para ejecutar el sistema TFM.
    """
    print("üîß SISTEMA MANTENIMIENTO PREDICTIVO TFM")
    print("=" * 50)

    try:
        # Inicializar sistema
        tfm_system = TFMPredictiveMaintenanceSystem()

        # Buscar archivo de datos por defecto
        candidates = [
            "data/datos_completos_tfm.csv",
            "data/datos_completos_tfm.xlsx",
            "datos_completos_tfm.csv",
            "datos_completos_tfm.xlsx",
        ]
        data_file = next((p for p in candidates if os.path.exists(p)), None)

        if not data_file:
            print("‚ùå No se encontr√≥ archivo de datos")
            print("üìÅ Archivos buscados:")
            for p in candidates:
                print(f"  - {p}")
            return

        # Ejecutar pipeline completo
        _ = tfm_system.run_complete_pipeline(data_file)

        print("\nüìÅ Archivos generados en directorio 'output/'")

    except Exception as e:
        print(f"‚ùå Error cr√≠tico: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
